#Este √© o mesmo modelo porem com llm groq
import os
import streamlit as st
from groq import Groq



# Configura√ß√µes da p√°gina
st.set_page_config(page_title="GPT do Michel (Groq)", page_icon="üß†")

# Inicializa√ß√£o do cliente Groq
client = Groq(api_key="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")

# ---------- ABA LATERAL ----------
st.sidebar.title("‚öôÔ∏è Op√ß√µes")
st.sidebar.markdown("Configure o comportamento da IA: Esta Intelig√™ncia Artificial foi projetada para responder perguntas sobre qualquer assunto de forma clara, √∫til e amig√°vel. ")
modelo = st.sidebar.selectbox("Modelo:", ["llama-3.3-70b-versatile"])  # Modelo atual do Groq
temperatura = st.sidebar.slider("Criatividade (temperature):", 0.0, 1.0, 0.7, 0.1)
st.sidebar.markdown("---")
st.sidebar.info("üí° Dica: quanto maior a temperatura, mais criativa a resposta.")

# ---------- T√çTULO PRINCIPAL ----------
st.markdown("<h1 style='text-align: center; color: #4F8BF9;'>ü§ñ GPT Michel com IA Groq</h1>", unsafe_allow_html=True)
st.write(" ")

# ---------- CAMPO DE ENTRADA ----------
prompt = st.chat_input("Digite sua pergunta para a IA...")

# Quando o usu√°rio envia uma pergunta
if prompt:
    with st.chat_message("user"):
        st.markdown(prompt)

    # Chamada para o modelo da Groq
    response = client.chat.completions.create(
        model=modelo,
        temperature=temperatura,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )

    resposta = response.choices[0].message.content

    with st.chat_message("assistant"):
        st.markdown(f"<div style='font-size: 18px;'>{resposta}</div>", unsafe_allow_html=True)

# ---------- RODAP√â ----------
st.markdown("""
<hr style="margin-top: 50px; margin-bottom: 10px;">
<p style='text-align: center; font-size: 0.8em; color: gray;'>
Desenvolvido por Michel com ‚ù§Ô∏è e Groq (Claude 3.5)
</p>
""", unsafe_allow_html=True)
